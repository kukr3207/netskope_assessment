{
  "url": "https://docs.netskope.com/en/cloud-exchange-faqs",
  "title": "Cloud Exchange FAQs - Netskope Knowledge Portal",
  "content": "Welcome to the Netskope Cloud Exchange (CE) FAQ page! This page answers common questions about understanding, installing, configuring, managing, and using Cloud Exchange. Our goal is to help you quickly find the information you need. If your question isn’t answered here, please refer to our Knowledge Base articles or the Troubleshooting Guide . Always consult the latest Netskope Cloud Exchange release notes and product documentation for the most current and detailed information. Last Updated: May 22, 2025 General Information & Core Concepts Q: What is Netskope Cloud Exchange? A: Netskope Cloud Exchange is a powerful platform service that enhances your security posture by enabling automated data exchange between your Netskope Security Cloud platform and your broader security and IT ecosystem. It allows you to share threat intelligence, risk scores, logs, and automate security workflows with third-party SIEMs, SOAR platforms, EDR solutions, ITSM tools, and more. Q: What are the main modules within Cloud Exchange and what data do they handle? A: Cloud Exchange includes several modules, each designed for specific data exchange functions: Log Shipper (CLS): Pulls logs (alerts, events, web traffic) from your Netskope tenant, transforms them, and shares them with your SIEM or log management platforms. It stores data temporarily on the CE machine’s disk only during the pulling and sharing process and does not store any data permanently. Threat Exchange (CTE): Facilitates bidirectional sharing of threat intelligence (like file hashes, malicious URLs/domains, IPs) between Netskope and other security tools (like threat intelligence platforms, EDRs). It stores indicators in its database. Ticket Orchestrator (CTO): Automates the creation and updating of tickets in ITSM or case management systems (e.g., ServiceNow, Jira) based on Netskope alerts. It stores alerts in its database. Risk Exchange (CREv2): Combines the capabilities of the original User Risk Exchange and Application Risk Exchange modules described below. User Risk Exchange (CRE for Users): Shares user and host risk scores (derived from Netskope User and Entity Behavior Analytics – UEBA) with other systems to enable risk-based access control and adaptive security policies. It stores these scores. Application Risk Exchange (CRE for Applications): Shares application risk scores (based on Netskope Cloud Confidence Index – CCI) with other platforms. It stores these application risk scores. Q: Can I revert Cloud Exchange or its plugins to a previous version? A: No, Cloud Exchange does not natively support downgrading or reverting the core Cloud Exchange platform or individual plugins to a previous version. Netskope highly recommends that you back up your Cloud Exchange configuration before performing any upgrades. If you need to run an older specific version, a fresh deployment of that version would typically be required, followed by a configuration restore if available and compatible. Q: How are Operating System (OS) updates managed for Cloud Exchange? A: Starting from Cloud Exchange v5.0.1, the platform is built on a Netskope PSIRT-hardened and reviewed Ubuntu OS. Because of this hardening, Cloud Exchange itself does not receive or prompt for general OS update notifications from the underlying OS repositories. For Cloud Exchange deployed as a Virtual Machine (VM/OVA), the underlying OS is updated to the latest hardened and reviewed version with each new official Cloud Exchange release. For containerized deployments on your own host, you are responsible for maintaining and securing the host OS, while Netskope maintains the OS within the Cloud Exchange containers. Q: Why does Cloud Exchange use user ID 1001 for its processes? A: Cloud Exchange uses a non-root user ID (such as 1001) for its internal worker processes primarily as a security best practice. This ensures the principle of least privilege, meaning processes run with only the permissions they absolutely need. This approach improves resource management within the containerized environment and contributes to a more secure overall deployment. This is an automatic internal process and does not require manual intervention. Installation & Deployment Q: What are the key processor architecture and feature requirements for installing Cloud Exchange? A: Cloud Exchange is designed and tested exclusively for the x86_64 architecture . A critical processor feature required is Advanced Vector Extensions (AVX) support. Machines must have both x86_64 architecture and AVX support for reliable operation. ARM64 Processors are Not Supported: Cloud Exchange does NOT support ARM64-based processors. This is because ARM64 processors do not typically support AVX, which is a fundamental requirement for Cloud Exchange. Running Cloud Exchange on ARM64 systems can lead to critical component failures or unreliable performance. For detailed system requirements (CPU cores, RAM, Disk space based on profiles like Small, Medium, Large), refer to Cloud Exchange System Requirements . Q: How do I configure or update proxy settings for Cloud Exchange after initial setup? A: For containerized deployments, you can configure or update proxy settings by re-running the setup script: Stop the running Cloud Exchange containers: sudo ./stop Go to your Cloud Exchange installation directory (e.g., cd /opt/cloudexchange/cloudexchange or cd ta_cloud_exchange ). Execute the setup script: sudo python3 ./setup When prompted during the script execution, enter your proxy details (like http://proxyserver.example.com:port ). Complete the rest of the setup questions. After the script completes, you can validate the proxy settings in the .env file within your installation directory: cat .env . Look for the CORE_HTTP_PROXY and CORE_HTTPS_PROXY variables. Start the Cloud Exchange containers: sudo ./start Cloud Exchange UI Option: You can also add/edit proxy details directly from the Cloud Exchange UI by going to Settings > General > Proxy . However, for these settings to be fully applied at the system level for all container communications, especially if Cloud Exchange itself needs to use the proxy for outbound connections from the host, re-running the ./setup script is often necessary to ensure the environment variables are correctly populated in the .env file used by the container runtime. Q: How can I deploy a specific previous (n-1) containerized version of Cloud Exchange using Git? A: To deploy a version prior to the latest (n-1 version), if you are managing your Cloud Exchange deployment via a Git repository: Go to your Cloud Exchange Git directory (e.g., ta_cloud_exchange ): cd <your_ce_directory> . Ensure your local repository is up-to-date with remote tags/branches: sudo git fetch --all --tags Check out the specific version tag you want to deploy. Version tags are typically in the format vX.Y.Z : sudo git checkout tags/ <version-number> . (Example: sudo git checkout tags/v5.1.0 ) Run the setup script to configure this version: sudo ./setup Start Cloud Exchange: sudo ./start Q: How do I upgrade my containerized Cloud Exchange from an older version to the latest using Git? A: To upgrade to the latest containerized version from an older one, if managing Cloud Exchange via Git: Go to your Cloud Exchange Git directory: cd <your_ce_directory> . Switch to the main branch (or the branch designated for latest stable releases, often ‘main’ or ‘master’): sudo git switch main Pull the latest updates from the repository: sudo git pull Run the setup script to apply any configuration changes for the new version: sudo ./setup Start Cloud Exchange: sudo ./start Always review the latest release notes for the new version for any specific upgrade instructions or important considerations before upgrading. Q: How do I regenerate self-signed SSL certificates for the Cloud Exchange web interface? A: If you need to regenerate the self-signed SSL certificates used by the Cloud Exchange web server (like if they expire, you want to change common name details, or for security reasons), follow these steps for a typical containerized deployment: Go to your main Cloud Exchange installation directory: (like /opt/cloudexchange/cloudexchange or ta_cloud_exchange ). Stop the Cloud Exchange services: sudo ./stop Go to the directory where SSL certificates are stored, usually: cd data/ssl_certs Remove or backup the old certificate and key files (like cte_cert.crt , cte_cert_key.key ): sudo rm -f cte_cert.crt cte_cert_key.key Return to the main Cloud Exchange installation directory: cd ../.. (if you were in data/ssl_certs) Run the setup script. The script should detect the missing certificates and guide you through generating new ones or generate them automatically: sudo python3 ./setup Start Cloud Exchange services: sudo ./start After regeneration, your browser will show a warning for the new self-signed certificate, which you’ll need to accept to access the UI. Q: What should I do if the Netskope REST API v2 token used by Cloud Exchange expires? A: If the API token expires, Cloud Exchange will lose its ability to communicate with your Netskope tenant for pulling data or sharing intelligence. You will typically see a warning banner in the Cloud Exchange UI stating something like: “The Netskope tenant API token has expired for [Tenant Name]. Generate the new token or re-issue the token and update the tenant configuration to resume communication between Netskope Tenant and Cloud Exchange.” To resolve this: In to your Netskope Tenant, go to Settings > Tools > REST API v2 . Locate the specific API v2 token that your Cloud Exchange instance is configured to use. Click the three dots (ellipsis icon) next to the token and select **Change Expiration**. Set a new, appropriate future expiration date. It’s also a good practice to reissue the token from the same menu. This generates a new token value. Copy this new token value. In Cloud Exchange UI, go to Settings > Netskope Tenants . Edit the configuration for your tenant and paste the newly reissued token value into the API Token field. Save the updated tenant configuration in Cloud Exchange. Q: Which virtualization platforms are supported for a Cloud Exchange VM deployment? A: Netskope provides Cloud Exchange VM images for several widely used virtualization platforms. Common supported formats and platforms include: VMware vSphere: using OVA – Open Virtualization Appliance format. Google Cloud Platform (GCP): often deployable using an OVA. Amazon Web Services (AWS): using AMI – Amazon Machine Image format. Microsoft Azure: using VHDX – Virtual Hard Disk format. Always consult the latest Netskope Cloud Exchange documentation for the specific supported platform versions and image formats for the Cloud Exchange version you intend to deploy. High Availability (HA) Deployment Q: Is Cloud Exchange High Availability (HA) supported for both VM and containerized deployments? A: Yes, the High Availability feature in Cloud Exchange is designed to work with standalone (containerized) deployments on Linux machines (such as Ubuntu or RHEL), as well as with Cloud Exchange deployed as a Virtual Machine (VM/OVA). Q: Can a Cloud Exchange HA cluster operate with nodes in different data centers (Multi-DC)? A: Yes, Cloud Exchange HA supports configurations where its instances (nodes) are running across multiple physical data centers or availability zones. Q: What is the recommended approach for deploying an HA cluster across multiple data centers (Multi-DC)? A: For a robust Multi-DC HA setup, consider the following: Deploy Cloud Exchange nodes in your different data centers, ensuring there is a stable and low-latency network connection between them. Utilize a centralized Network File System (NFS) server. This NFS server should be located in a primary or highly available location and be accessible by all CE nodes. It’s used for sharing data like configurations and databases to ensure consistency across the cluster. Ensure each Cloud Exchange node has reliable network connectivity to the primary node (or the node currently managing shared resources like the NFS mount) for proper failover handling and data synchronization. Thoroughly validate inter-DC connectivity, redundancy of network paths, NFS server availability, and overall performance (especially network latency) between the Cloud Exchange instances in different data centers. Q: What are the key network requirements for a Cloud Exchange node in a secondary site connecting to an NFS mount in the primary site for HA? A: The critical network requirements include: Reliability & Stability: The network link must be stable to prevent data corruption or split-brain scenarios in the HA cluster. Low Latency: Network response time (latency) between the Cloud Exchange nodes (especially those in secondary DCs) and the central NFS server should be minimal. High latency can significantly degrade performance and slow down failover times. Sufficient Bandwidth: The network bandwidth must be adequate to handle the data read/write operations required by Cloud Exchange for all its modules, particularly during peak loads or data synchronization events between nodes and the NFS share. Q: Are performance issues expected if Cloud Exchange HA nodes are deployed in different data centers (Multi-DC)? A: It’s possible. Compared to an HA setup where all nodes are within the same data center (and thus very low latency), a Multi-DC HA setup might experience slightly increased operational delays (like a few seconds for certain operations or failover, due to the inherent network latency between geographically separated data centers. Performance can be affected by: Inter-data center network latency. Packet loss on the links between data centers. Bandwidth limitations between the sites. Careful network design, capacity planning for inter-DC links, appropriate load balancing strategies in front of the Cloud Exchange cluster, and rigorous failover testing are crucial to identify and mitigate potential performance impacts in a Multi-DC HA deployment. Q: How does failover and traffic handling typically work in a Multi-DC HA setup for Cloud Exchange? A: Cloud Exchange HA is designed for automatic failover. If an active node in the cluster becomes unresponsive or fails, the HA mechanism should detect this condition. It will then coordinate to promote a healthy standby node (which could be in another data center) to take over active operations. Traffic redirection to the new active node might depend on your external load balancer configuration (if used) in front of the Cloud Exchange cluster, or DNS updates, or client-side retry mechanisms in integrated tools. Comprehensive testing for Multi-DC HA usually involves scenarios like: Simulating node failures (like powering off a VM, stopping Cloud Exchange services, or restarting containers on one node). Validating data consistency and integrity across all nodes and on the shared NFS storage after a failover. Testing read/write operations to the shared storage (NFS) from all active and standby nodes. Verifying the HA synchronization status and the health of the cluster communication mechanisms. Q: Which Cloud Exchange modules (CLS, CTE, CTO, CRE) are typically verified during Multi-DC HA testing? A: Validation is generally performed to ensure uninterrupted or quickly restored workflow for all key Cloud Exchange modules during and after failover events in a Multi-DC HA setup. This includes: CLS (Cloud Log Shipper): Ensuring continuous data ingestion from Netskope and forwarding to third-party SIEMs without significant data loss or excessive duplication. CTE (Cloud Threat Exchange): Verifying seamless sharing, synchronization, and application of threat intelligence across the cluster and with integrated security tools. CTO (Cloud Ticket Orchestrator): Testing that automated ticket creation, updates, and synchronization with ITSM systems continue to function correctly and without creating erroneous duplicates. CRE (Cloud Risk Exchange): Ensuring that user, host, and application risk scores continue to be assessed, shared, and available for consumption by integrated systems. Log Shipper (CLS) Q: How far back can Log Shipper retrieve historical alerts from the Netskope tenant? A: The Netskope platform typically stores detailed alerts in its active database for a period of around three months. Log Shipper’s historical pulling feature can retrieve alerts from within this available retention window in the Netskope tenant. Q: How can I perform a historical pull of older logs using the Log Shipper module? A: Log Shipper includes a feature to pull historical alert and event data. To use this: In the Cloud Exchange UI, go to the Log Shipper module from the main menu. Within Log Shipper, go to the SIEM Mappings section (or a similarly named section responsible for configuring data destinations). On the SIEM Mappings page, look for Pull Historical Data , where you can select a Date Range to pull historical data. Open the dialog by clicking the icon on the SIEM Mappings page (near the list of existing mappings or rules). In the Pull Historical Data dialog, select the desired start and end date range for which you want to retrieve logs, and then initiate the pull process. Important Considerations: Pulling a large volume of historical data can be time-consuming and may place a load on both the Netskope tenant and your Cloud Exchange instance. Plan accordingly. Depending on your SIEM’s ingestion settings and how it handles timestamps, pulling historical data might lead to log duplication if those logs were already partially ingested, or if the time ranges overlap with ongoing streaming. Q: How can I stop an ongoing historical log pull in Log Shipper if needed? A: Yes. If you need to stop an active historical pulling process in Log Shipper, the general method is to restart the Cloud Exchange services. This can be done via the Command Line Interface (CLI) on the Cloud Exchange host by going to the Cloud Exchange installation directory and running sudo ./stop followed by sudo ./start . Q: Is it possible to perform a historical pull of Web Transaction (WebTx) logs using Log Shipper? A: No. Historical pulling of detailed WebTx logs for an arbitrary, user-defined extended time range (like several weeks or months back) is generally not possible through the standard Log Shipper historical pull feature. This limitation is due to how Web Transaction events are stored by Netskope’s ingestion services (like in systems such as Google PubSub Lite for recent data), which typically retain this highly granular transaction data for a shorter period (like the last 7 days). While Cloud Exchange subscribes to this feed for near real-time and recent WebTx logs, it cannot access WebTx data further back than the retention period of the specific source system. For long-term storage, analysis, and historical querying of WebTx logs, it is essential to have Log Shipper continuously streaming these events to your SIEM or log management platform as they occur. For more detailed information, refer to Netskope Transaction Events . Q: What are the types of Alerts and Events that Log Shipper can typically fetch from the Netskope tenant? A: Log Shipper is capable of fetching a comprehensive range of alerts and events generated by the Netskope Security Cloud Platform. Common examples include: Alerts: Compromised Credential alerts, Policy violation alerts (for various policies like web access, cloud app usage, etc.), Malsite access alerts, Malware detection alerts, Data Loss Prevention (DLP) incidents, Security Assessment findings (misconfigurations), Quarantine actions, Remediation events, User and Entity Behavior Analytics (UBA) anomalies and alerts, Watchlist alerts, and Threat Exchange alerts. Events: Detailed Page access events, Application usage events, Audit logs (tracking administrative actions within the Netskope platform), Infrastructure-related events, Network events (if applicable to your Netskope services), and overarching Security Incidents that group related alerts/events. The exact list of available log types can evolve. Always refer to the latest Cloud Exchange and Netskope platform documentation for the most up-to-date list of supported log and event types for Log Shipper with each 3rd-party plugin. Threat Exchange (CTE) Q: Where in the Cloud Exchange UI can I view the threat indicators (IoCs) received by the Threat Exchange module? A: You can view the threat indicators that Threat Exchange has processed and stored by going to Threat Exchange (in the left-side panel) and then selecting Threat IoCs . This page displays a sortable and filterable table listing the IoCs, including their value, type (like URL, MD5, SHA256, IP), Netskope Hits, and hits from other sources. Q: How can I identify the original source of a specific Threat IoC listed in Cloud Exchange? A: On the Threat Exchange > Threat IoCs page, the UI provides filtering capabilities. To find IoCs from a particular source (like a specific plugin, such as Proofpoint or CrowdStrike), you can use the filter builder. The underlying query language format might be something like [sources.source Like \"plugin_identifier\"] or similar, which you can often construct using the UI’s filter interface by selecting the source field and specifying the plugin name or identifier. Q: How can I create and manage tags for IoCs within the Threat Exchange module? A: Tags help categorize and manage IoCs. You can create and manage these tags by going to Threat Exchange (in the left-side panel) and then selecting Manage Tags . You can enter a tag name (like High_Confidence_Malware_URL ) and add it. Existing tags are also listed on this page with options to edit or delete them. Q: Which versions of Netskope API tokens are required for the Threat Exchange module to operate correctly? A: For full functionality, including both pulling threat intelligence from Netskope (like policy violations that generate IoCs) and pushing external threat intelligence into Netskope (for policy enforcement), the Threat Exchange module generally requires API tokens for both Netskope REST API v1 and REST API v2. Ensure both are correctly configured in Cloud Exchange under Settings > Netskope Tenants with the necessary permissions. Q: How do I create a deduplication or exception rule in Threat Exchange to prevent processing redundant IoCs? A: To create rules that prevent redundant processing of IoCs (like excluding certain IoCs, or IoCs from specific sources from being shared further), you can configure this in a Business Rule in the Threat Exchange module: In the Cloud Exchange UI, go to Threat Exchange > Business Rules . Either create a new Business Rule, or edit an existing one. In the Business Rule configuration, after defining your main criteria for IoC selection, look for the option to add Exception Rules or similar conditional logic. Expand a primary rule (like URL Only), and then clicking the + icon next to Exception Rules. In the exception rule, you can define the criteria for IoCs that should be excluded from the actions of that Business Rule. Q: Does Threat Exchange fetch IoCs directly from the Netskope tenant UI under Incident > Malware? A: No. The Threat Exchange module primarily sources IoCs from Netskope based on specific Alerts generated by the platform, particularly Skope IT Malware Alerts and Malsite Alerts. It doesn’t typically pull IoCs directly from a generalized Incident > Malware overview page if that page represents a different data aggregation or view within the Netskope tenant UI. To see the types of alerts that Cloud Exchange would process for IoC generation, go to your Netskope tenant’s main Alerts page, then use the filter options to select the Alert Type as Malsite and Malware , and then click Apply . Q: What is the purpose of the Aging Criteria setting in a Threat Exchange plugin configuration? A: The Aging Criteria (sometimes referred to as Time To Live or TTL for indicators) in Threat Exchange determines how long Cloud Exchange will continue to consider an indicator as active after the original source platform (the third-party tool integrated via that plugin) stops reporting it or deletes it. Instead of immediately removing the IoC from its own database and from Netskope policies, Cloud Exchange will age it out based on this configured duration period (the default is often 90 days). This prevents IoCs from going in and out of policies if a source temporarily stops sending it, or if there’s a desire to keep it active for a period for historical context or extended protection, even if the original source no longer sees it as an active threat. Ticket Orchestrator (CTO) Q: Can the Ticket Orchestrator module use Netskope Events (in addition to Alerts) to create tickets in an ITSM system? A: No, the primary design of the Ticket Orchestrator module is to process Alerts generated by your Netskope tenant. Based on these alerts and the business rules you configure in Cloud Exchange, it then creates or updates corresponding tickets in your integrated ITSM system (like ServiceNow, Jira, etc.). It does not typically use raw Netskope Events (which are more granular activity logs) for direct ticket creation. Q: How can I prevent Ticket Orchestrator from creating duplicate tickets for the same recurring Netskope alert? A: Ticket Orchestrator includes mechanisms for alert deduplication to avoid creating multiple ITSM tickets for the same underlying security issue or alert. You can configure these deduplication settings within the Business Rules or global settings for the Ticket Orchestrator module. This involves defining criteria based on specific alert fields (like alert name, affected user/resource, and threat name) and a time window. If a new alert matches an existing one based on your criteria, Ticket Orchestrator can update an existing ticket instead of creating a new one. Q: What is the default retention period for alerts stored locally by Cloud Exchange for the Ticket Orchestrator module, and can this be changed? A: For the purpose of ticket tracking and deduplication, the Ticket Orchestrator module stores alert information locally within Cloud Exchange. The default retention period for these stored alerts is typically 7 days. And yes, this retention period can be modified in the Cloud Exchange UI by going to Settings > Ticket Orchestrator (or a similar path for module-specific settings) and adjusting the value for alert retention to suit your operational needs. Risk Exchange (CREv2) Q: From which part of the Netskope platform does Cloud Exchange’s Risk Exchange module pull user and host risk scores? A: The Risk Exchange module in Cloud Exchange primarily pulls user and host risk scores that are generated by Netskope’s User and Entity Behavior Analytics (UEBA) capabilities. Specifically, it often sources this information from Skope IT UBA Alerts within your Netskope tenant. To view these types of alerts directly in your Netskope tenant, go to the main Alerts page, and then use the filter options to select Alert Type as UBA , and then click Apply . Q: Is a specific Netskope license required to fetch user risk scores using the Risk Exchange module? A: Yes. To enable the generation of user risk scores within the Netskope platform, and make them available for Cloud Exchange’s Risk Exchange module to retrieve and share, an Advanced UEBA (User and Entity Behavior Analytics) license is generally required for your Netskope tenant. Still have questions? If you couldn’t find the answer you were looking for, check our Knowledge Base , review the Troubleshooting Guide , or contact Netskope Support .",
  "content_length": 27705,
  "crawl_timestamp": 1750780393.66544
}